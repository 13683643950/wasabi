#-----------------------------------------------------------------
#----- Defult global configuration for all spark-applications ----
#-----------------------------------------------------------------
default {
  ## Spark common configuration for all spark applications. These can be overridden in application specific configs
  spark {

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=1g

    #Amount of memory to use per executor process
    spark.executor.memory=2g

    #Number of cores to use per executor process
    spark.cores.max=2

    #Port for your application's dashboard, which shows memory and workload data.
    #spark.ui.port=4041

    #Port for the driver to listen on. This is used for communicating with the executors and the standalone Master.
    #spark.driver.port=4040

    #To run locally
    #spark.driver.host=localhost

    #Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.
    #spark.eventLog.enabled=true

    #spark.eventLog.dir="cfs:/spark/events"

    #Whether to compress logged events, if spark.eventLog.enabled is true.
    #spark.eventLog.compress=true

    #Spark Clean-up:
    #spark.history.fs.cleaner.enabled
    #spark.history.fs.cleaner.interval
    #spark.history.fs.cleaner.maxAge

  }

  datastore {
    type=cassandra
    cluster=default
    host=localhost
    port=9042
    keyspace=wasabi_experiments
  }
}

## Configurations specific to the migrate-data spark appliction
migrate-data {

  ## spark specific configurations
  spark {
    spark.app.name=migrate-data

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=8g

    #Amount of memory to use per executor process
    spark.executor.memory=24g

    #Number of cores to use per executor process
    spark.executor.cores=8


    #Spark-Cassandra-Connector configurations

    #spark.cassandra.connection.keep_alive_ms
    #spark.cassandra.connection.timeout_ms
    #spark.driver.maxResultSize

    #Following two sets of properties are useful to throtle SAPRK migration application

    #Write speed throtle:
    #cassandra.output.batch.size.bytes
    #cassandra.output.batch.size.rows
    #cassandra.output.concurrent.writes
    #cassandra.output.throughput_mb_per_sec (decrease write rate to avoid write timeouts)

    #Read speed throtle:
    #spark.cassandra.input.fetch.size_in_rows
    #spark.cassandra.input.split.size_in_mb

  }

  ## migrations specific configurations
  migration {

    #Format:
    #datastore=<Name of the supported datastore - this is a global config to migrate-data/migration and so can be used across processors>
    #host=<datastore host name - this is a global config to migrate-data/migration and so can be used across processors>
    #port=<datastore port - this is a global config to migrate-data/migration and so can be used across processors>
    #keyspace=<datastore keyspace/database name - this is a global config to migrate-data/migration and so can be used across processors>
    #
    #processors=<List the processor IDs to be excuted in the specified order. in the format processorId1,processorId2,...,processorIdN.
    #           <For each processorId at least processorId.class config is mandatory.>
    #processorId1.class=<Fully qualified classpath to the MigrationProcessor Class e.g. com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor>
    #processorId2.class=<Fully qualified classpath to the MigrationProcessor Class>
    #processorIdN.class=<Fully qualified classpath to the MigrationProcessor Class>
    #
    #<Following configs are only applied to CopyTableMigrationProcessor. And similarly any other custom processor can have their specific configurations as needed>
    #processorId1.src.table=<Source table name>
    #processorId1.dest.table=<Destination table name>
    #processorId1.src.select_projection=<This can be used to rename coulumns, delete columns, add calculated columns etc. Default value is '*' >
    #processorId1.src.where_clause=<This would be used to filter unncessory data from source table. Default value is empty means no filtering>
    #processorId1.dest.saveMode=<SaveMode implicitly apply for destination table.>
    #<Following convention can be used to specify source & destination specific datastore, host, port, keyspace>
    #processorId1.src.keyspace=<Override the $keyspace config value and it is used to specify source keyspace. >
    #processorId1.dest.keyspace=<Override the $keyspace config value and it is used to specify destination keyspace>
    #


    datastores {
      src.type=${default.datastore.type}
      src.cluster=${default.datastore.cluster}
      src.host=${default.datastore.host}
      src.port=${default.datastore.port}
      src.keyspace=${default.datastore.keyspace}

      dest.type=${default.datastore.type}
      dest.cluster=${default.datastore.cluster}
      dest.host=${default.datastore.host}
      dest.port=${default.datastore.port}
      dest.keyspace=${default.datastore.keyspace}
    }

    #List processor ids, separated by '-', which needs to be executed as a part of migrate-data application
    #processors=CopyUserAssignmentTblProcessor-CopyExperimentTblProcessor
    processors=CopyUserAssignmentTblProcessor

    CopyExperimentTblProcessor {
      class = com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor
      src.table = "experiment"
      src.select_projection = "*"
      src.where_clause = ""
      dest.table = "experiment"
      dest.saveMode = Append
    }

    CopyUserAssignmentTblProcessor {
      class = com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor
      src.table = "user_assignment"
      src.select_projection = "experiment_id,user_id,context,bucket_label,created"
      src.where_clause = "created <= wasabi_current_timestamp('GMT')"
      # unix_timestamp input format yyyy-MM-dd HH:mm:ss
      #src.where_clause = "created <= to_utc_timestamp('2016-10-10 11:26:17', 'GMT')"

      dest.table = "user_assignment_lookup"
      dest.saveMode = Append
    }
  }
}


## Configurations specific to the execute-sql spark appliction
execute-sql {
  spark {
    spark.app.name=execute-sql

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    #spark.driver.memory=8g

    #Amount of memory to use per executor process
    #spark.executor.memory=48g

    #Number of cores to use per executor process
    #spark.cores.max=8

    spark.cassandra.input.fetch.size_in_rows=2000000
    spark.cassandra.input.split.size_in_mb=200

    #500k, 50mb => 9.7 min
    #spark.cassandra.input.fetch.size_in_rows=500000
    #spark.cassandra.input.split.size_in_mb=52428800

    #1.2m, 100mb => 6.8 min
    #spark.cassandra.input.fetch.size_in_rows=1200000
    #spark.cassandra.input.split.size_in_mb=104857600

    #2.4m, 200mb => Got OperationTimedOutException(s)
    #spark.cassandra.input.fetch.size_in_rows=2400000
    #spark.cassandra.input.split.size_in_mb=209715200

  }

  sql="select * from experiment"

  limit=100

  repartition=0

  #StorageLevel="MEMORY_ONLY"
  StorageLevel=""

  save=false
}

## Configurations specific to the populate-data spark appliction
populate-data {
  spark {
    spark.app.name=populate-data

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    #spark.driver.memory=1g

    #Amount of memory to use per executor process
    #spark.executor.memory=1g

    #Number of cores to use per executor process
    spark.cores.max=8

  }

  table="user_assignment"

  #Format: num_experiments-number_of_user_assignments_per_experiments-num_batches_or_rdd_partitions
  #Data will be generated per batch/rdd_partition. Each batch size is 100K
  #allocation=4-36427520 1-29903188 1-25553634 1-19572996 3-15767136 9-8155415 104-1522344 277-163108
  allocation=1-1000

  #Data will be generated per batch/rdd_partition. Each batch size is 100K
  batchSize=100

}
