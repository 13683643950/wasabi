#-----------------------------------------------------------------
#----- Defult global configuration for all spark-applications ----
#-----------------------------------------------------------------
default {
  ## Spark common configuration for all spark applications. These can be overridden in application specific configs
  spark {

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=1g

    #Amount of memory to use per executor process
    spark.executor.memory=2g

    #Number of cores to use per executor process
    spark.executor.cores=1

    #Port for your application's dashboard, which shows memory and workload data.
    spark.ui.port=4041

    #Port for the driver to listen on. This is used for communicating with the executors and the standalone Master.
    spark.driver.port=4040

    #To run locally
    spark.driver.host=localhost

    #Whether to log Spark events, useful for reconstructing the Web UI after the application has finished.
    spark.eventLog.enabled=false

    #Whether to compress logged events, if spark.eventLog.enabled is true.
    spark.eventLog.compress=true

    #Spark Clean-up:
    #spark.history.fs.cleaner.enabled
    #spark.history.fs.cleaner.interval
    #spark.history.fs.cleaner.maxAge

  }

  ## Cassandra common configurations for all spark applications. These can be overridden in application specific configs
  cassandra {
    #Global spark - cassandra connection properties
    spark.cassandra.connection.host=localhost
    spark.cassandra.connection.port=9042
    spark.cassandra.connection.keyspace=wasabi_experiments
  }
}

## Configurations specific to the migrate-data spark appliction
migrate-data {

  ## spark specific configurations
  spark {
    spark.app.name=migrate-data

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=8g

    #Amount of memory to use per executor process
    spark.executor.memory=24g

    #Number of cores to use per executor process
    spark.executor.cores=8


    #Spark-Cassandra-Connector configurations

    #spark.cassandra.connection.keep_alive_ms
    #spark.cassandra.connection.timeout_ms
    #spark.driver.maxResultSize

    #Following two sets of properties are useful to throtle SAPRK migration application

    #Write speed throtle:
    #cassandra.output.batch.size.bytes
    #cassandra.output.batch.size.rows
    #cassandra.output.concurrent.writes
    #cassandra.output.throughput_mb_per_sec (decrease write rate to avoid write timeouts)

    #Read speed throtle:
    #spark.cassandra.input.fetch.size_in_rows
    #spark.cassandra.input.split.size_in_mb

  }

  ## migrations specific configurations
  migration {

    #Format:
    #datastore=<Name of the supported datastore - this is a global config to migrate-data/migration and so can be used across processors>
    #host=<datastore host name - this is a global config to migrate-data/migration and so can be used across processors>
    #port=<datastore port - this is a global config to migrate-data/migration and so can be used across processors>
    #keyspace=<datastore keyspace/database name - this is a global config to migrate-data/migration and so can be used across processors>
    #
    #processors=<List the processor IDs to be excuted in the specified order. in the format processorId1,processorId2,...,processorIdN.
    #           <For each processorId at least processorId.class config is mandatory.>
    #processorId1.class=<Fully qualified classpath to the MigrationProcessor Class e.g. com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor>
    #processorId2.class=<Fully qualified classpath to the MigrationProcessor Class>
    #processorIdN.class=<Fully qualified classpath to the MigrationProcessor Class>
    #
    #<Following configs are only applied to CopyTableMigrationProcessor. And similarly any other custom processor can have their specific configurations as needed>
    #processorId1.src.table=<Source table name>
    #processorId1.dest.table=<Destination table name>
    #processorId1.src.select_projection=<This can be used to rename coulumns, delete columns, add calculated columns etc. Default value is '*' >
    #processorId1.src.where_clause=<This would be used to filter unncessory data from source table. Default value is empty means no filtering>
    #processorId1.dest.saveMode=<SaveMode implicitly apply for destination table.>
    #<Following convention can be used to specify source & destination specific datastore, host, port, keyspace>
    #processorId1.src.keyspace=<Override the $keyspace config value and it is used to specify source keyspace. >
    #processorId1.dest.keyspace=<Override the $keyspace config value and it is used to specify destination keyspace>
    #


    datastoreType=cassandra
    cluster=default
    host=localhost
    port=9042
    keyspace=wasabi_experiments

    DataStores {
      src.datastoreType=${migrate-data.migration.datastoreType}
      src.cluster=${migrate-data.migration.cluster}
      src.host=${migrate-data.migration.host}
      src.port=${migrate-data.migration.port}
      src.keyspace=${migrate-data.migration.keyspace}

      dest.datastoreType=${migrate-data.migration.datastoreType}
      dest.cluster=${migrate-data.migration.cluster}
      dest.host=${migrate-data.migration.host}
      dest.port=${migrate-data.migration.port}
      dest.keyspace=${migrate-data.migration.keyspace}
    }

    #List processor ids, separated by '-', which needs to be executed as a part of migrate-data application
    #processors=CopyUserAssignmentTblProcessor-CopyExperimentTblProcessor
    processors=CopyUserAssignmentTblProcessor

    CopyExperimentTblProcessor {
      class = com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor
      src.table = "experiment"
      src.select_projection = "*"
      src.where_clause = ""
      dest.table = "experiment"
      dest.saveMode = Append
    }

    CopyUserAssignmentTblProcessor {
      class = com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor
      src.table = "user_assignment"
      src.select_projection = "experiment_id,user_id,context,bucket_label,created"
      #src.where_clause = "created <= wasabi_current_timestamp('GMT')"
      # unix_timestamp input format yyyy-MM-dd HH:mm:ss
      #src.where_clause = "created <= to_utc_timestamp('2016-10-10 11:26:17', 'GMT')"

      dest.table = "user_assignment_lookup"
      dest.saveMode = Append
    }
  }
}

## Configurations specific to the daily-aggregation spark appliction
daily-aggregation {
  spark {
    spark.app.name=daily-aggregation

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=8g

    #Amount of memory to use per executor process
    spark.executor.memory=48g

    #Number of cores to use per executor process
    spark.executor.cores=8
  }
}

## Configurations specific to the daily-aggregation spark appliction
execute-sql {
  spark {
    spark.app.name=execute-sql

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=8g

    #Amount of memory to use per executor process
    spark.executor.memory=48g

    #Number of cores to use per executor process
    spark.executor.cores=8
  }

  datstore {
    type=cassandra
    cluster=default
    host=localhost
    port=9042
    keyspace=wasabi_experiments
  }

}

