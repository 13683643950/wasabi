#-----------------------------------------------------------------
#----- Defult global configuration for all spark-applications ----
#-----------------------------------------------------------------
default {
  ## Spark common configuration for all spark applications. These can be overridden in application specific configs
  spark {

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=1g

    #Amount of memory to use per executor process
    spark.executor.memory=2g

    #Number of cores to use per executor process
    spark.cores.max=2

    #Limit of total size of serialized results of all partitions for each Spark action (e.g. collect). Default is 1gb.
    #spark.driver.maxResultSize=1g

    #------------------------------------------------------------------------------------------------------------------
    #Spark-Cassandra-Connector configurations
    #------------------------------------------------------------------------------------------------------------------
    #Period of time to keep unused connections open. Default is 5000ms. Change it to 5mins
    spark.cassandra.connection.keep_alive_ms=300000
    #Maximum period of time to attempt connecting to a node. Default is 5000ms. Change it to 5mins
    spark.cassandra.connection.timeout_ms=300000
    #Maximum period of time to wait for a read to return. Default is 120000. Change it to 1min
    spark.cassandra.read.timeout_ms=60000
    #The local DC to connect to (other nodes will be ignored)
    #spark.cassandra.connection.local_dc=dc_dse489_spark

  }

  datastore {
    type=cassandra
    cluster=default
    host=localhost
    port=9042
    keyspace=wasabi_experiments
  }
}

## Configurations specific to the migrate-data spark appliction
migrate-data {

  ## spark specific configurations
  spark {
    spark.app.name=migrate-data

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    spark.driver.memory=8g

    #Amount of memory to use per executor process
    spark.executor.memory=48g

    #Number of cores to use per executor process. Change to 4 so total cores would be 4*12 => 48
    spark.cores.max=48

    #Fraction of Java heap to use for aggregation and cogroups during shuffles, if spark.shuffle.spill is true. Default is 0.2
    spark.shuffle.memoryFraction=0.6

    #Fraction of Java heap to use for Spark's memory cache. Default is 0.6
    spark.storage.memoryFraction=0.2

    #------------------------------------------------------------------------------------------------------------------
    #Following two sets of properties are useful to throtle SAPRK migration application
    #------------------------------------------------------------------------------------------------------------------
    #Write speed tuning/throtle:
    #Number of rows per single batch; default is 'auto' which means the connector will adjust the number of rows based on the amount of data in each row.
    #spark.cassandra.output.batch.size.rows=auto

    #Maximum total size of the batch in bytes; defaults to 1 kB. Chnage to 10mb
    spark.cassandra.output.batch.size.bytes=10485760

    #How many batches per single Spark task can be stored in memory before sending to Cassandra; default 1000. Change to 500 so maximum would be 500*10mb*48 => 5000mb*8 => 40000mb => ~40gb per node
    spark.cassandra.output.batch.grouping.buffer.size=500

    #Determines how insert statements are grouped into batches; Available values are: none, replica_set, partition (default)
    #spark.cassandra.output.batch.grouping.key=partition

    #Maximum number of batches executed in parallel by a single Spark task; defaults to 5. Keep it 5
    spark.cassandra.output.concurrent.writes=10

    #Consistency level for writing
    #spark.cassandra.spark.cassandra.output.consistency.level=LOCAL_QUORUM

    #Sets whether to record connector specific metrics on write. DFefault is true.
    #spark.cassandra.output.metrics=true

    #Maximum write throughput allowed per single core in MB/s limit this on long (+8 hour) runs to 70% of your max throughput as seen on a smaller job for stability.
    # Default value is 2.147483647E9.
    #spark.cassandra.output.throughput_mb_per_sec=10

    #------------------------------------------------------------------------------------------------------------------
    #Read speed tuning/throtle:

    #Consistency level to use when reading. Default is LOCAL_ONE
    #spark.cassandra.consistency.level=LOCAL_ONE

    #Number of CQL rows fetched per roundtrip, default 1000. Change it to 200,000
    spark.cassandra.input.fetch.size_in_rows=200000

    #Approx amount of data to be fetched into a single Spark partition, default 64 MB. Chnage to 128mb.
    #Minimum number of resulting Spark partitions is 1 + 2 * SparkContext.defaultParallelism
    spark.cassandra.input.split.size_in_mb=128

    #Sets whether to record connector specific metrics on write. Default is true.
    #spark.cassandra.input.metrics=true

    #Maximum read throughput allowed per single core in query/s while joining RDD with Cassandra table
    #spark.cassandra.input.join.throughput_query_per_sec=9223372036854775807

  }

  ## migrations specific configurations
  migration {

    #Format:
    #datastore=<Name of the supported datastore - this is a global config to migrate-data/migration and so can be used across processors>
    #host=<datastore host name - this is a global config to migrate-data/migration and so can be used across processors>
    #port=<datastore port - this is a global config to migrate-data/migration and so can be used across processors>
    #keyspace=<datastore keyspace/database name - this is a global config to migrate-data/migration and so can be used across processors>
    #
    #processors=<List the processor IDs to be excuted in the specified order. in the format processorId1,processorId2,...,processorIdN.
    #           <For each processorId at least processorId.class config is mandatory.>
    #processorId1.class=<Fully qualified classpath to the MigrationProcessor Class e.g. com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor>
    #processorId2.class=<Fully qualified classpath to the MigrationProcessor Class>
    #processorIdN.class=<Fully qualified classpath to the MigrationProcessor Class>
    #
    #<Following configs are only applied to CopyTableMigrationProcessor. And similarly any other custom processor can have their specific configurations as needed>
    #processorId1.src.table=<Source table name>
    #processorId1.dest.table=<Destination table name>
    #processorId1.src.select_projection=<This can be used to rename coulumns, delete columns, add calculated columns etc. Default value is '*' >
    #processorId1.src.where_clause=<This would be used to filter unncessory data from source table. Default value is empty means no filtering>
    #processorId1.dest.saveMode=<SaveMode implicitly apply for destination table.>
    #<Following convention can be used to specify source & destination specific datastore, host, port, keyspace>
    #processorId1.src.keyspace=<Override the $keyspace config value and it is used to specify source keyspace. >
    #processorId1.dest.keyspace=<Override the $keyspace config value and it is used to specify destination keyspace>
    #


    datastores {
      src.type=${default.datastore.type}
      src.cluster=${default.datastore.cluster}
      src.host=${default.datastore.host}
      src.port=${default.datastore.port}
      src.keyspace=${default.datastore.keyspace}

      dest.type=${default.datastore.type}
      dest.cluster=${default.datastore.cluster}
      dest.host=${default.datastore.host}
      dest.port=${default.datastore.port}
      dest.keyspace=${default.datastore.keyspace}
    }

    #List processor ids, separated by '-', which needs to be executed as a part of migrate-data application
    #processors=CopyUserAssignmentTblProcessor-CopyExperimentTblProcessor
    processors=CopyUserAssignmentTblProcessor

    CopyUserAssignmentTblProcessor {
      class = com.intuit.wasabi.data.processor.migratedata.CopyTableMigrationProcessor
      src.table = "user_assignment"
      src.select_projection = "experiment_id,user_id,context,bucket_label,created"
      src.where_clause = "created <= wasabi_current_timestamp('GMT')"
      # unix_timestamp input format yyyy-MM-dd HH:mm:ss
      #src.where_clause = "created <= to_utc_timestamp('2016-10-10 11:26:17', 'GMT')"

      dest.table = "user_assignment_lookup"
      dest.saveMode = Append
    }
  }
}


## Configurations specific to the execute-sql spark appliction
execute-sql {
  spark {
    spark.app.name=execute-sql

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    #spark.driver.memory=8g

    #Amount of memory to use per executor process
    #spark.executor.memory=48g

    #Number of cores to use per executor process
    #spark.cores.max=8

    spark.cassandra.input.fetch.size_in_rows=2000000
    spark.cassandra.input.split.size_in_mb=200

    #500k, 50mb => 9.7 min
    #spark.cassandra.input.fetch.size_in_rows=500000
    #spark.cassandra.input.split.size_in_mb=52428800

    #1.2m, 100mb => 6.8 min
    #spark.cassandra.input.fetch.size_in_rows=1200000
    #spark.cassandra.input.split.size_in_mb=104857600

    #2.4m, 200mb => Got OperationTimedOutException(s)
    #spark.cassandra.input.fetch.size_in_rows=2400000
    #spark.cassandra.input.split.size_in_mb=209715200

  }

  sql="select * from experiment"

  limit=100

  repartition=0

  #StorageLevel="MEMORY_ONLY"
  StorageLevel=""

  save=false
}

## Configurations specific to the populate-data spark appliction
populate-data {
  spark {
    spark.app.name=populate-data

    #Amount of memory to use for the driver process, i.e. where SparkContext is initialized.
    #spark.driver.memory=1g

    #Amount of memory to use per executor process
    #spark.executor.memory=1g

    #Number of cores to use per executor process
    spark.cores.max=8

  }

  table="user_assignment"

  #Format: num_experiments-number_of_user_assignments_per_experiments-num_batches_or_rdd_partitions
  #Data will be generated per batch/rdd_partition. Each batch size is 100K
  #allocation=4-36427520 1-29903188 1-25553634 1-19572996 3-15767136 9-8155415 104-1522344 277-163108
  allocation=1-1000

  #Data will be generated per batch/rdd_partition. Each batch size is 100K
  batchSize=100

}
